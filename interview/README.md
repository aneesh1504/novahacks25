# Voice Interviewer

## Description
Voice Interviewer is a minimalist, browser-based mock interview companion built with React that speaks each question aloud, records your answers, visualizes the waveform in real time, and transcribes what you say. The experience is fully local: questions are delivered with neural text-to-speech (XTTS) generated by a small Flask backend, while your microphone audio is captured via the Web Audio API, converted to 16 kHz mono WAV, and transcribed offline with Vosk. When you finish, the app can export the full transcript so you can review both prompts and answers.

## Tech Stack
- **Frontend:** React 18 + Vite with custom glassmorphism UI, Canvas-based Siri-style blob, and the Web Audio API/MediaRecorder/AudioWorklet stack.
- **Speech Recognition:** Vosk offline speech-to-text model running inside the bundled Flask server.
- **Neural Text-to-Speech:** XTTS via the ElevenLabs Python SDK (or optional browser `speechSynthesis` fallback).
- **Backend:** Flask (Python 3) serving both static assets and `/transcribe` + `/tts` endpoints.
- **Tooling:** Lightweight static build (no bundler/runtime required); runs anywhere Python and a modern browser are available.

## Installation
Requirements:
- Python 3.10+
- Node.js 18+ (for the React build)
- A modern Chromium/Firefox/Safari browser with `getUserMedia`, `AudioContext`, and `MediaRecorder`.
- (Optional) ElevenLabs API key for neural TTS (`ELEVENLABS_API_KEY` env var).

1. **Clone and enter the project**
   ```zsh
   git clone <this-repo>
   cd novahacks25/interview
   ```

2. **Install frontend dependencies and build the React app**
   ```zsh
   npm install
   npm run build
   ```

3. **Download a Vosk English model**
   - Download from https://alphacephei.com/vosk/models (e.g., `vosk-model-small-en-us-0.15`).
   - Unzip into `models/vosk-model-small-en-us-0.15`.
   - If you place it elsewhere, set `VOSK_MODEL=/absolute/path/to/model`.

4. **Create a Python virtual environment and install backend dependencies**
   ```zsh
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r server/requirements.txt
   ```

5. **(Optional) Enable neural TTS**
   ```zsh
   export ELEVENLABS_API_KEY="your_api_key"
   ```

## Run
1. Start the Flask server (it serves the built React frontend from `dist/`):
   ```zsh
   source .venv/bin/activate
   python server/app.py
   ```
   - The first call to the TTS endpoint downloads the XTTS model (~2 GB).
   - Server defaults to `http://localhost:5173`; override with the `PORT` env var if needed.

2. Open `http://localhost:5173` in your browser:
   - Allow microphone access.
   - Choose **XTTS (Neural Voice)** for ElevenLabs playback or use the browser TTS fallback.
   - Click **Start Interview** to begin recording; use **Export Transcript (txt)** when finished.

### Development mode
- In one terminal run the backend: `python server/app.py` (defaults to port 5173).
- In a second terminal run `npm run dev`. The Vite dev server starts on port **5174** and proxies `/transcribe` + `/tts` to the Flask backend, so API calls continue to succeed without CORS issues.
- Use `BACKEND_PORT` or `FRONTEND_PORT` env vars if you need to customize either port (the proxy picks up these values automatically).

## Project Layout
- `index.html` – Vite entry for the React single-page experience.
- `src/` – React components, hooks, and styles (`App.jsx`, `components/LiquidBlob.jsx`, `hooks/useInterviewEngine.js`, etc.).
- `public/worklets/recorder-worklet.js` – AudioWorklet processor that streams PCM frames for offline STT.
- `server/app.py` – Flask server exposing `/transcribe` (Vosk) and `/tts` (XTTS) endpoints.
- `models/` – Place downloaded Vosk models here.
- `ARCHITECTURE.md`, `QUICKSTART.md`, `ELEVENLABS_SETUP.md` – Additional docs detailing design and voice configuration.
